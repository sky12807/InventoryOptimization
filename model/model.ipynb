{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import BasicBlock,conv1x1,conv3x3\n",
    "\n",
    "from model.unet_parts import *\n",
    "from model.layers import Attention\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels,grid_dim, T, pre_dim = 1,bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        self.conv_grid = nn.Sequential(\n",
    "            norm_layer(grid_dim),\n",
    "            nn.Conv2d(grid_dim, 32, kernel_size=7, stride=1, padding=3,\n",
    "                               bias=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False),\n",
    "                               )\n",
    "        \n",
    "        \n",
    "        self.bn0 = norm_layer(n_channels)\n",
    "        self.n_channels = n_channels\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "\n",
    "        self.h0 = nn.Parameter(torch.randn(1,1,64))\n",
    "        self.c0 = nn.Parameter(torch.randn(1,1,64))\n",
    "        self.rnn = nn.LSTM(n_channels,64,num_layers=1,batch_first=True)\n",
    "        \n",
    "        self.inc = DoubleConv(64, 64)\n",
    "        \n",
    "        \n",
    "    \n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 128)\n",
    "        self.up3 = Up(256, 64, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        \n",
    "        self.conv_y_s = nn.ModuleList()\n",
    "        self.out_conv_s = nn.ModuleList()\n",
    "        self.pre_dim = pre_dim\n",
    "        for _ in range(pre_dim):\n",
    "            self.conv_y_s.append(nn.Sequential(\n",
    "                                nn.Conv2d(1, 8, kernel_size=7, stride=1, padding=3,\n",
    "                                                   bias=False),\n",
    "                                nn.LeakyReLU(),\n",
    "                                nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1,\n",
    "                                                   bias=False),\n",
    "                                nn.LeakyReLU(),\n",
    "                                nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1,\n",
    "                                                   bias=False),\n",
    "                                                   )\n",
    "                        )\n",
    "            \n",
    "            self.out_conv_s.append(nn.Sequential(nn.Conv2d(64+8+8, 64, kernel_size=1), #(8+8+8,64),\n",
    "                                                nn.LeakyReLU(),\n",
    "                                                nn.Conv2d(64, 1, kernel_size=1))\n",
    "                                  )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x,grid,yt_1):\n",
    "        \n",
    "        #x B*T*C*H*W\n",
    "        B,T,C,H,W = x.shape\n",
    "        print(f'Initial x shape: {x.shape}')\n",
    "        x = x.reshape(B*T,C,H,W)\n",
    "        print(f'Reshape x shape: {x.shape}')\n",
    "        x = self.bn0(x)\n",
    "        print(f'Batch Norm shape: {x.shape}')\n",
    "        x = x.reshape(B,T,C,H,W)\n",
    "        print(f'Reshape x shape: {x.shape}')\n",
    "        \n",
    "        x = x.permute(0,3,4,1,2) #x B*H*W*T*C\n",
    "        print(f'Permute x shape: {x.shape}')\n",
    "        x = x.reshape(B*H*W,T,C)\n",
    "        print(f'Reshape x shape: {x.shape}')\n",
    "        h0 = self.h0.repeat(1,B*H*W,1)\n",
    "        c0 = self.c0.repeat(1,B*H*W,1)\n",
    "        print(f'h0 shape: {h0.shape}')\n",
    "        print(f'c0 shape: {c0.shape}')\n",
    "        _,(hn,cn) = self.rnn(x,(h0,c0)) #hn num_layers*(B*H*W)*rnn_hidden\n",
    "        print(f'hn shape: {hn.shape}')\n",
    "        print(f'cn shape: {cn.shape}')\n",
    "        x = hn[-1]\n",
    "        print(f'x shape: {}')\n",
    "        x = x.reshape(B,H,W,-1)\n",
    "        x = x.permute(0,3,1,2)\n",
    "        x = x.reshape(B,-1,H,W)\n",
    "        \n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "#         x4 = self.down3(x3)\n",
    "#         x5 = self.down4(x4)\n",
    "#         x = self.up1(x5, x4)\n",
    "#         x = self.up2(x, x3)\n",
    "        x = self.up3(x3, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        \n",
    "#         logits = self.outc(x) #B*T,_,H,W\n",
    "        logits = x.reshape(B,-1,H,W)\n",
    "#         logits = logits.permute(2,3,0,1)\n",
    "#         logits = logits.view(W,H,-1)\n",
    "        \n",
    "        #grid:B*27*H*W\n",
    "        grid = self.conv_grid(grid)\n",
    "#         grid = grid.permute(2,3,0,1)\n",
    "#         grid = grid.view(W,H,-1)\n",
    "        \n",
    "        \n",
    "        #yt_1:B*pre_dim*H*W        \n",
    "        logits = torch.cat([logits,grid],dim = 1)\n",
    "        \n",
    "        out = []\n",
    "        for air in range(self.pre_dim):\n",
    "            yt_1_now = self.conv_y_s[air](yt_1[:,air:air+1])\n",
    "            out.append(self.out_conv_s[air](torch.cat([logits,yt_1_now],dim = 1)))\n",
    "            \n",
    "        out = torch.cat(out,dim = 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_Res(nn.Module):\n",
    "    def __init__(self, n_channels,grid_dim, T, bilinear=True):\n",
    "        super(UNet_Res, self).__init__()\n",
    "        \n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        self.conv_grid = nn.Sequential(\n",
    "            norm_layer(grid_dim),\n",
    "            nn.Conv2d(grid_dim, 32, kernel_size=7, stride=1, padding=3,\n",
    "                               bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False),\n",
    "                               )\n",
    "        \n",
    "        self.inplanes = 64\n",
    "        self.bn0 = norm_layer(n_channels)\n",
    "        self.conv0 = nn.Conv2d(n_channels, self.inplanes, kernel_size=7, stride=1, padding=3,\n",
    "                               bias=False)\n",
    "        \n",
    "        self.bilinear = bilinear\n",
    "        \n",
    "#         self.inc = DoubleConv(n_channels, 64)\n",
    "        self.inc = nn.Sequential(\n",
    "            BasicBlock(64, 64),\n",
    "#             BasicBlock(64, 64)\n",
    "        )\n",
    "        \n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 128)\n",
    "#         self.down2 = Down(128, 256)\n",
    "#         self.down3 = Down(256, 512)\n",
    "#         self.down4 = Down(512, 512)\n",
    "#         self.up1 = Up(1024, 256, bilinear)\n",
    "#         self.up2 = Up(512, 128, bilinear)\n",
    "#         self.up3 = Up(256, 64, bilinear)\n",
    "        self.up3 = Up(256, 64, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, 1)\n",
    "                \n",
    "        self.outt = nn.Sequential(OutConv(64, 1))\n",
    "\n",
    "        \n",
    "    def forward(self, x,grid):\n",
    "        x = self.bn0(x)\n",
    "        x = self.conv0(x)\n",
    "        \n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "#         x4 = self.down3(x3)\n",
    "#         x5 = self.down4(x4)\n",
    "#         x = self.up1(x5, x4)\n",
    "#         x = self.up2(x, x3)\n",
    "        x = self.up3(x3, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        \n",
    "        logits = self.outc(x)\n",
    "        T,_,W,H = logits.shape\n",
    "        logits = logits.permute(0,2,3,0)\n",
    "        logits = logits.view(W,H,-1)\n",
    "        \n",
    "        grid = self.conv_grid(grid)\n",
    "        grid = grid.permute(2,3,0,1)\n",
    "        grid = grid.view(W,H,-1)\n",
    "        \n",
    "        logits = torch.cat([logits,grid],dim = -1)\n",
    "        logits = self.outt(logits)\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
